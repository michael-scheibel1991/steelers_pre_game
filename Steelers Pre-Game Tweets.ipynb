{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steelers Pre-Game Tweets\n",
    "**By:** _Mike Scheibel_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy import OAuthHandler\n",
    "from tweepy.streaming import StreamListener\n",
    "import tweepy\n",
    "import json\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "import string\n",
    "import os\n",
    "import time\n",
    "from Mike_API_Keys import api_key, api_key_secret, access_token, access_token_secret\n",
    "from datetime import datetime\n",
    "#from scraping_tweets import scraptweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(api_key,api_key_secret)\n",
    "auth.set_access_token(access_token,access_token_secret)\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_tweets = pd.DataFrame(columns = ['username', 'acctdesc', 'location', 'following',\n",
    "                                    'followers', 'totaltweets', 'usercreatedts', 'tweetcreatedts',\n",
    "                                    'retweetcount', 'text', 'hashtags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_words = \"#HereWeGo OR #Steelers\"\n",
    "date_since = \"2020-11-19\"\n",
    "numTweets = 1000\n",
    "numRuns = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of tweets scraped for run 0 is 1000\n",
      "time take for 0 run to complete is 54.95\n",
      "no. of tweets scraped for run 1 is 1000\n",
      "time take for 1 run to complete is 54.88\n",
      "no. of tweets scraped for run 2 is 1000\n",
      "time take for 2 run to complete is 755.71\n",
      "no. of tweets scraped for run 3 is 1000\n",
      "time take for 3 run to complete is 55.51\n",
      "no. of tweets scraped for run 4 is 1000\n",
      "time take for 4 run to complete is 56.08\n",
      "no. of tweets scraped for run 5 is 1000\n",
      "time take for 5 run to complete is 720.34\n",
      "no. of tweets scraped for run 6 is 1000\n",
      "time take for 6 run to complete is 57.57\n",
      "no. of tweets scraped for run 7 is 1000\n",
      "time take for 7 run to complete is 57.32\n",
      "no. of tweets scraped for run 8 is 1000\n",
      "time take for 8 run to complete is 717.87\n",
      "no. of tweets scraped for run 9 is 1000\n",
      "time take for 9 run to complete is 59.99\n",
      "no. of tweets scraped for run 10 is 1000\n",
      "time take for 10 run to complete is 750.89\n",
      "no. of tweets scraped for run 11 is 1000\n",
      "time take for 11 run to complete is 60.38\n",
      "no. of tweets scraped for run 12 is 1000\n",
      "time take for 12 run to complete is 59.57\n",
      "no. of tweets scraped for run 13 is 1000\n",
      "time take for 13 run to complete is 713.51\n",
      "no. of tweets scraped for run 14 is 1000\n",
      "time take for 14 run to complete is 61.57\n",
      "no. of tweets scraped for run 15 is 1000\n",
      "time take for 15 run to complete is 61.82\n",
      "no. of tweets scraped for run 16 is 1000\n",
      "time take for 16 run to complete is 708.84\n",
      "no. of tweets scraped for run 17 is 1000\n",
      "time take for 17 run to complete is 64.5\n",
      "no. of tweets scraped for run 18 is 1000\n",
      "time take for 18 run to complete is 746.13\n",
      "no. of tweets scraped for run 19 is 1000\n",
      "time take for 19 run to complete is 64.79\n",
      "no. of tweets scraped for run 20 is 1000\n",
      "time take for 20 run to complete is 65.51\n",
      "no. of tweets scraped for run 21 is 1000\n",
      "time take for 21 run to complete is 703.55\n",
      "no. of tweets scraped for run 22 is 1000\n",
      "time take for 22 run to complete is 66.34\n",
      "no. of tweets scraped for run 23 is 1000\n",
      "time take for 23 run to complete is 66.99\n",
      "no. of tweets scraped for run 24 is 1000\n",
      "time take for 24 run to complete is 698.62\n",
      "no. of tweets scraped for run 25 is 1000\n",
      "time take for 25 run to complete is 68.8\n",
      "no. of tweets scraped for run 26 is 1000\n",
      "time take for 26 run to complete is 744.05\n",
      "no. of tweets scraped for run 27 is 1000\n",
      "time take for 27 run to complete is 71.13\n",
      "no. of tweets scraped for run 28 is 1000\n",
      "time take for 28 run to complete is 72.16\n",
      "no. of tweets scraped for run 29 is 1000\n",
      "time take for 29 run to complete is 690.36\n",
      "no. of tweets scraped for run 30 is 1000\n",
      "time take for 30 run to complete is 74.25\n",
      "no. of tweets scraped for run 31 is 1000\n",
      "time take for 31 run to complete is 75.19\n",
      "no. of tweets scraped for run 32 is 1000\n",
      "time take for 32 run to complete is 683.32\n",
      "no. of tweets scraped for run 33 is 1000\n",
      "time take for 33 run to complete is 77.92\n",
      "no. of tweets scraped for run 34 is 1000\n",
      "time take for 34 run to complete is 734.73\n",
      "no. of tweets scraped for run 35 is 1000\n",
      "time take for 35 run to complete is 78.73\n",
      "no. of tweets scraped for run 36 is 1000\n",
      "time take for 36 run to complete is 79.85\n",
      "no. of tweets scraped for run 37 is 1000\n",
      "time take for 37 run to complete is 674.25\n",
      "no. of tweets scraped for run 38 is 1000\n",
      "time take for 38 run to complete is 82.21\n",
      "no. of tweets scraped for run 39 is 1000\n",
      "time take for 39 run to complete is 82.89\n",
      "no. of tweets scraped for run 40 is 1000\n",
      "time take for 40 run to complete is 671.24\n",
      "no. of tweets scraped for run 41 is 1000\n",
      "time take for 41 run to complete is 85.83\n",
      "no. of tweets scraped for run 42 is 1000\n",
      "time take for 42 run to complete is 724.33\n",
      "no. of tweets scraped for run 43 is 1000\n",
      "time take for 43 run to complete is 86.49\n",
      "no. of tweets scraped for run 44 is 1000\n",
      "time take for 44 run to complete is 88.53\n",
      "no. of tweets scraped for run 45 is 1000\n",
      "time take for 45 run to complete is 658.83\n",
      "no. of tweets scraped for run 46 is 1000\n",
      "time take for 46 run to complete is 90.81\n",
      "no. of tweets scraped for run 47 is 1000\n",
      "time take for 47 run to complete is 91.15\n",
      "no. of tweets scraped for run 48 is 1000\n",
      "time take for 48 run to complete is 652.55\n",
      "no. of tweets scraped for run 49 is 1000\n",
      "time take for 49 run to complete is 93.68\n",
      "no. of tweets scraped for run 50 is 1000\n",
      "time take for 50 run to complete is 94.93\n",
      "no. of tweets scraped for run 51 is 1000\n",
      "time take for 51 run to complete is 647.45\n",
      "no. of tweets scraped for run 52 is 1000\n",
      "time take for 52 run to complete is 97.9\n",
      "no. of tweets scraped for run 53 is 1000\n",
      "time take for 53 run to complete is 712.72\n",
      "no. of tweets scraped for run 54 is 1000\n",
      "time take for 54 run to complete is 99.0\n",
      "no. of tweets scraped for run 55 is 1000\n",
      "time take for 55 run to complete is 101.04\n",
      "no. of tweets scraped for run 56 is 1000\n",
      "time take for 56 run to complete is 634.21\n",
      "no. of tweets scraped for run 57 is 1000\n",
      "time take for 57 run to complete is 102.26\n",
      "no. of tweets scraped for run 58 is 1000\n",
      "time take for 58 run to complete is 103.44\n",
      "no. of tweets scraped for run 59 is 1000\n",
      "time take for 59 run to complete is 628.65\n",
      "no. of tweets scraped for run 60 is 1000\n",
      "time take for 60 run to complete is 105.9\n",
      "no. of tweets scraped for run 61 is 1000\n",
      "time take for 61 run to complete is 705.21\n",
      "no. of tweets scraped for run 62 is 1000\n",
      "time take for 62 run to complete is 108.53\n",
      "no. of tweets scraped for run 63 is 1000\n",
      "time take for 63 run to complete is 108.37\n",
      "no. of tweets scraped for run 64 is 1000\n",
      "time take for 64 run to complete is 617.38\n",
      "no. of tweets scraped for run 65 is 1000\n",
      "time take for 65 run to complete is 110.44\n",
      "no. of tweets scraped for run 66 is 1000\n",
      "time take for 66 run to complete is 111.69\n",
      "no. of tweets scraped for run 67 is 1000\n",
      "time take for 67 run to complete is 613.89\n",
      "no. of tweets scraped for run 68 is 1000\n",
      "time take for 68 run to complete is 114.82\n",
      "no. of tweets scraped for run 69 is 1000\n",
      "time take for 69 run to complete is 695.05\n",
      "no. of tweets scraped for run 70 is 1000\n",
      "time take for 70 run to complete is 115.56\n",
      "no. of tweets scraped for run 71 is 1000\n",
      "time take for 71 run to complete is 116.81\n",
      "no. of tweets scraped for run 72 is 1000\n",
      "time take for 72 run to complete is 601.86\n",
      "no. of tweets scraped for run 73 is 1000\n",
      "time take for 73 run to complete is 118.61\n",
      "no. of tweets scraped for run 74 is 1000\n",
      "time take for 74 run to complete is 119.32\n",
      "no. of tweets scraped for run 75 is 1000\n",
      "time take for 75 run to complete is 596.97\n",
      "no. of tweets scraped for run 76 is 1000\n",
      "time take for 76 run to complete is 121.2\n",
      "no. of tweets scraped for run 77 is 1000\n",
      "time take for 77 run to complete is 689.8\n",
      "no. of tweets scraped for run 78 is 1000\n",
      "time take for 78 run to complete is 124.46\n",
      "no. of tweets scraped for run 79 is 1000\n",
      "time take for 79 run to complete is 125.18\n",
      "no. of tweets scraped for run 80 is 1000\n",
      "time take for 80 run to complete is 584.96\n",
      "no. of tweets scraped for run 81 is 1000\n",
      "time take for 81 run to complete is 128.48\n",
      "no. of tweets scraped for run 82 is 1000\n",
      "time take for 82 run to complete is 128.37\n",
      "no. of tweets scraped for run 83 is 1000\n",
      "time take for 83 run to complete is 578.61\n",
      "no. of tweets scraped for run 84 is 1000\n",
      "time take for 84 run to complete is 129.05\n",
      "no. of tweets scraped for run 85 is 1000\n",
      "time take for 85 run to complete is 681.66\n",
      "no. of tweets scraped for run 86 is 1000\n",
      "time take for 86 run to complete is 132.0\n",
      "no. of tweets scraped for run 87 is 1000\n",
      "time take for 87 run to complete is 132.36\n",
      "no. of tweets scraped for run 88 is 1000\n",
      "time take for 88 run to complete is 570.24\n",
      "no. of tweets scraped for run 89 is 1000\n",
      "time take for 89 run to complete is 134.72\n",
      "no. of tweets scraped for run 90 is 1000\n",
      "time take for 90 run to complete is 135.45\n",
      "no. of tweets scraped for run 91 is 1000\n",
      "time take for 91 run to complete is 563.02\n",
      "no. of tweets scraped for run 92 is 1000\n",
      "time take for 92 run to complete is 137.1\n",
      "no. of tweets scraped for run 93 is 1000\n",
      "time take for 93 run to complete is 138.6\n",
      "no. of tweets scraped for run 94 is 1000\n",
      "time take for 94 run to complete is 557.0\n",
      "no. of tweets scraped for run 95 is 1000\n",
      "time take for 95 run to complete is 140.61\n",
      "no. of tweets scraped for run 96 is 1000\n",
      "time take for 96 run to complete is 672.2\n",
      "no. of tweets scraped for run 97 is 1000\n",
      "time take for 97 run to complete is 142.67\n",
      "no. of tweets scraped for run 98 is 1000\n",
      "time take for 98 run to complete is 144.12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of tweets scraped for run 99 is 1000\n",
      "time take for 99 run to complete is 546.72\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, numRuns):\n",
    "    # We will time how long it takes to scrape tweets for each run:\n",
    "    start_run = time.time()\n",
    "\n",
    "    # Collect tweets using the Cursor object\n",
    "    # .Cursor() returns an object that you can iterate or loop over to access the data collected.\n",
    "    # Each item in the iterator has various attributes that you can access to get information about each tweet\n",
    "    tweets = tweepy.Cursor(api.search, q=search_words, lang=\"en\", since=date_since, tweet_mode='extended').items(numTweets)\n",
    "\n",
    "    # Store these tweets into a python list\n",
    "    tweet_list = [tweet for tweet in tweets]\n",
    "\n",
    "    # Obtain the following info (methods to call them out):\n",
    "        # user.screen_name - twitter handle\n",
    "        # user.description - description of account\n",
    "        # user.location - where is he tweeting from\n",
    "        # user.friends_count - no. of other users that user is following (following)\n",
    "        # user.followers_count - no. of other users who are following this user (followers)\n",
    "        # user.statuses_count - total tweets by user\n",
    "        # user.created_at - when the user account was created\n",
    "        # created_at - when the tweet was created\n",
    "        # retweet_count - no. of retweets\n",
    "        # tweet.entities['hashtags'] - hashtags in the tweet\n",
    "\n",
    "    # Begin scraping the tweets individually:\n",
    "    noTweets = 0\n",
    "\n",
    "    for tweet in tweet_list:\n",
    "\n",
    "        # Pull the values\n",
    "        username = tweet.user.screen_name\n",
    "        acctdesc = tweet.user.description\n",
    "        location = tweet.user.location\n",
    "        following = tweet.user.friends_count\n",
    "        followers = tweet.user.followers_count\n",
    "        totaltweets = tweet.user.statuses_count\n",
    "        usercreatedts = tweet.user.created_at\n",
    "        tweetcreatedts = tweet.created_at\n",
    "        retweetcount = tweet.retweet_count\n",
    "        hashtags = tweet.entities['hashtags']\n",
    "\n",
    "        try:\n",
    "            text = tweet.retweeted_status.full_text\n",
    "        except AttributeError:  # Not a Retweet\n",
    "            text = tweet.full_text\n",
    "\n",
    "        # Add the 11 variables to the empty list - ith_tweet:\n",
    "        ith_tweet = [username, acctdesc, location, following, followers, totaltweets,\n",
    "                     usercreatedts, tweetcreatedts, retweetcount, text, hashtags]\n",
    "\n",
    "        # Append to dataframe - db_tweets\n",
    "        db_tweets.loc[len(db_tweets)] = ith_tweet\n",
    "\n",
    "        # increase counter - noTweets  \n",
    "        noTweets += 1\n",
    "\n",
    "    # Run ended:\n",
    "    end_run = time.time()\n",
    "    duration_run = round(end_run-start_run, 2)\n",
    "\n",
    "    print('no. of tweets scraped for run {} is {}'.format(i, noTweets))\n",
    "    print('time take for {} run to complete is {}'.format(i, duration_run))\n",
    "\n",
    "    time.sleep(30) #30second sleep time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>acctdesc</th>\n",
       "      <th>location</th>\n",
       "      <th>following</th>\n",
       "      <th>followers</th>\n",
       "      <th>totaltweets</th>\n",
       "      <th>usercreatedts</th>\n",
       "      <th>tweetcreatedts</th>\n",
       "      <th>retweetcount</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BrayGod</td>\n",
       "      <td>the white Wesley Snipes</td>\n",
       "      <td>Area 51</td>\n",
       "      <td>435</td>\n",
       "      <td>466</td>\n",
       "      <td>14100</td>\n",
       "      <td>2012-06-30 04:48:35</td>\n",
       "      <td>2020-11-22 04:44:45</td>\n",
       "      <td>274</td>\n",
       "      <td>Facts! #Steelers https://t.co/TASbwulZsQ</td>\n",
       "      <td>[{'text': 'Steelers', 'indices': [26, 35]}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>elviabloop</td>\n",
       "      <td>ucla‚Äô24 (‚Åé‚ÅçÃ¥Ãõ·¥ó‚ÅçÃ¥Ãõ‚Åé)</td>\n",
       "      <td>Earthland</td>\n",
       "      <td>343</td>\n",
       "      <td>120</td>\n",
       "      <td>2687</td>\n",
       "      <td>2016-06-23 04:45:25</td>\n",
       "      <td>2020-11-22 04:44:36</td>\n",
       "      <td>274</td>\n",
       "      <td>Facts! #Steelers https://t.co/TASbwulZsQ</td>\n",
       "      <td>[{'text': 'Steelers', 'indices': [26, 35]}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>teemeeboy21</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>178</td>\n",
       "      <td>133</td>\n",
       "      <td>3463</td>\n",
       "      <td>2014-06-18 12:34:04</td>\n",
       "      <td>2020-11-22 04:43:25</td>\n",
       "      <td>490</td>\n",
       "      <td>MOOD! #Steelers https://t.co/DBlYuddYFk</td>\n",
       "      <td>[{'text': 'Steelers', 'indices': [24, 33]}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DerikMcCourtney</td>\n",
       "      <td>el camino driver</td>\n",
       "      <td></td>\n",
       "      <td>141</td>\n",
       "      <td>49</td>\n",
       "      <td>746</td>\n",
       "      <td>2014-05-25 01:32:50</td>\n",
       "      <td>2020-11-22 04:43:13</td>\n",
       "      <td>29</td>\n",
       "      <td>Stay tuned because tomorrow we will be giving ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>iGotGasFYTB</td>\n",
       "      <td>People ChangeüåìMoney Remains The Same NLMBüíúüì≤ Fo...</td>\n",
       "      <td></td>\n",
       "      <td>2024</td>\n",
       "      <td>1505</td>\n",
       "      <td>39742</td>\n",
       "      <td>2017-12-16 05:45:42</td>\n",
       "      <td>2020-11-22 04:43:11</td>\n",
       "      <td>362</td>\n",
       "      <td>Landed in Jacksonville üõ¨ , back at it tomorrow...</td>\n",
       "      <td>[{'text': 'HereWeGo', 'indices': [83, 92]}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>PittpanthersML</td>\n",
       "      <td>University of Pittsburgh alum. Pitt season tic...</td>\n",
       "      <td>Pittsburgh, PA</td>\n",
       "      <td>4101</td>\n",
       "      <td>1844</td>\n",
       "      <td>43367</td>\n",
       "      <td>2010-11-01 18:34:43</td>\n",
       "      <td>2020-11-22 11:57:47</td>\n",
       "      <td>0</td>\n",
       "      <td>Good morning Steeler Nation!  Let‚Äôs Go!  #Here...</td>\n",
       "      <td>[{'text': 'HereWeGo', 'indices': [41, 50]}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>RealMcMinn</td>\n",
       "      <td>Huge Pittsburgh Steelers fan.  WWE disciple.  ...</td>\n",
       "      <td>Mississippi, USA</td>\n",
       "      <td>241</td>\n",
       "      <td>15</td>\n",
       "      <td>1048</td>\n",
       "      <td>2018-05-28 21:26:21</td>\n",
       "      <td>2020-11-22 11:57:23</td>\n",
       "      <td>0</td>\n",
       "      <td>@AlannaWoolridge Its ass whooping day!!\\n#Here...</td>\n",
       "      <td>[{'text': 'HereWeGo', 'indices': [40, 49]}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>pierce_delray</td>\n",
       "      <td>steelers 4 life , love the city ,pull for the ...</td>\n",
       "      <td>Virginia, USA</td>\n",
       "      <td>1324</td>\n",
       "      <td>790</td>\n",
       "      <td>12486</td>\n",
       "      <td>2015-05-04 15:35:12</td>\n",
       "      <td>2020-11-22 11:55:21</td>\n",
       "      <td>35</td>\n",
       "      <td>Happy birthday to the young goat JuJu, a team ...</td>\n",
       "      <td>[{'text': 'Steelers', 'indices': [75, 84]}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>AustinHavlin</td>\n",
       "      <td>Bellarmine 2019. RRT at St. Elizabeth.</td>\n",
       "      <td>Southgate, KY</td>\n",
       "      <td>528</td>\n",
       "      <td>133</td>\n",
       "      <td>2113</td>\n",
       "      <td>2014-02-16 02:32:31</td>\n",
       "      <td>2020-11-22 11:54:57</td>\n",
       "      <td>1201</td>\n",
       "      <td>The #Steelers play the Jags on Sunday so let's...</td>\n",
       "      <td>[{'text': 'Steelers', 'indices': [23, 32]}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>BiasedGirl</td>\n",
       "      <td>We are not going to agree on everything... and...</td>\n",
       "      <td>Where I need to be</td>\n",
       "      <td>7617</td>\n",
       "      <td>30287</td>\n",
       "      <td>490999</td>\n",
       "      <td>2008-12-18 01:41:13</td>\n",
       "      <td>2020-11-22 11:51:43</td>\n",
       "      <td>1</td>\n",
       "      <td>Matchup To Watch: Steelers Vs Jaguars -  #Here...</td>\n",
       "      <td>[{'text': 'HereWeGo', 'indices': [60, 69]}, {'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows √ó 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              username                                           acctdesc  \\\n",
       "0              BrayGod                            the white Wesley Snipes   \n",
       "1           elviabloop                                ucla‚Äô24 (‚Åé‚ÅçÃ¥Ãõ·¥ó‚ÅçÃ¥Ãõ‚Åé)   \n",
       "2          teemeeboy21                                                      \n",
       "3      DerikMcCourtney                                   el camino driver   \n",
       "4          iGotGasFYTB  People ChangeüåìMoney Remains The Same NLMBüíúüì≤ Fo...   \n",
       "...                ...                                                ...   \n",
       "99995   PittpanthersML  University of Pittsburgh alum. Pitt season tic...   \n",
       "99996       RealMcMinn  Huge Pittsburgh Steelers fan.  WWE disciple.  ...   \n",
       "99997    pierce_delray  steelers 4 life , love the city ,pull for the ...   \n",
       "99998     AustinHavlin             Bellarmine 2019. RRT at St. Elizabeth.   \n",
       "99999       BiasedGirl  We are not going to agree on everything... and...   \n",
       "\n",
       "                 location following followers totaltweets       usercreatedts  \\\n",
       "0                 Area 51       435       466       14100 2012-06-30 04:48:35   \n",
       "1               Earthland       343       120        2687 2016-06-23 04:45:25   \n",
       "2                               178       133        3463 2014-06-18 12:34:04   \n",
       "3                               141        49         746 2014-05-25 01:32:50   \n",
       "4                              2024      1505       39742 2017-12-16 05:45:42   \n",
       "...                   ...       ...       ...         ...                 ...   \n",
       "99995      Pittsburgh, PA      4101      1844       43367 2010-11-01 18:34:43   \n",
       "99996    Mississippi, USA       241        15        1048 2018-05-28 21:26:21   \n",
       "99997       Virginia, USA      1324       790       12486 2015-05-04 15:35:12   \n",
       "99998       Southgate, KY       528       133        2113 2014-02-16 02:32:31   \n",
       "99999  Where I need to be      7617     30287      490999 2008-12-18 01:41:13   \n",
       "\n",
       "           tweetcreatedts retweetcount  \\\n",
       "0     2020-11-22 04:44:45          274   \n",
       "1     2020-11-22 04:44:36          274   \n",
       "2     2020-11-22 04:43:25          490   \n",
       "3     2020-11-22 04:43:13           29   \n",
       "4     2020-11-22 04:43:11          362   \n",
       "...                   ...          ...   \n",
       "99995 2020-11-22 11:57:47            0   \n",
       "99996 2020-11-22 11:57:23            0   \n",
       "99997 2020-11-22 11:55:21           35   \n",
       "99998 2020-11-22 11:54:57         1201   \n",
       "99999 2020-11-22 11:51:43            1   \n",
       "\n",
       "                                                    text  \\\n",
       "0               Facts! #Steelers https://t.co/TASbwulZsQ   \n",
       "1               Facts! #Steelers https://t.co/TASbwulZsQ   \n",
       "2                MOOD! #Steelers https://t.co/DBlYuddYFk   \n",
       "3      Stay tuned because tomorrow we will be giving ...   \n",
       "4      Landed in Jacksonville üõ¨ , back at it tomorrow...   \n",
       "...                                                  ...   \n",
       "99995  Good morning Steeler Nation!  Let‚Äôs Go!  #Here...   \n",
       "99996  @AlannaWoolridge Its ass whooping day!!\\n#Here...   \n",
       "99997  Happy birthday to the young goat JuJu, a team ...   \n",
       "99998  The #Steelers play the Jags on Sunday so let's...   \n",
       "99999  Matchup To Watch: Steelers Vs Jaguars -  #Here...   \n",
       "\n",
       "                                                hashtags  \n",
       "0            [{'text': 'Steelers', 'indices': [26, 35]}]  \n",
       "1            [{'text': 'Steelers', 'indices': [26, 35]}]  \n",
       "2            [{'text': 'Steelers', 'indices': [24, 33]}]  \n",
       "3                                                     []  \n",
       "4            [{'text': 'HereWeGo', 'indices': [83, 92]}]  \n",
       "...                                                  ...  \n",
       "99995        [{'text': 'HereWeGo', 'indices': [41, 50]}]  \n",
       "99996        [{'text': 'HereWeGo', 'indices': [40, 49]}]  \n",
       "99997        [{'text': 'Steelers', 'indices': [75, 84]}]  \n",
       "99998        [{'text': 'Steelers', 'indices': [23, 32]}]  \n",
       "99999  [{'text': 'HereWeGo', 'indices': [60, 69]}, {'...  \n",
       "\n",
       "[100000 rows x 11 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping has completed!\n"
     ]
    }
   ],
   "source": [
    "# Once all runs have completed, save them to a single csv file:    \n",
    "# Obtain timestamp in a readable format:\n",
    "\n",
    "to_csv_timestamp = datetime.today().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Define working path and filename\n",
    "path = os.getcwd()\n",
    "filename = to_csv_timestamp + '_steelers_tweets.csv'\n",
    "\n",
    "# Store dataframe in csv with creation date timestamp\n",
    "db_tweets.to_csv(filename, index = False)\n",
    "\n",
    "print('Scraping has completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "\n",
    "from pprint import pprint # get some prettier printing of objects\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sw = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = open(\"20201122_070537_steelers_tweets.csv\", encoding='utf-8').read()\n",
    "\n",
    "sd_clean = [w for w in sd.lower().split()]\n",
    "sd_clean = [w.lower() for w in sd_clean if w.isalpha() and w not in sw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': 831459,\n",
       " 'unique_tokens': 4412,\n",
       " 'avg_token_length': 5.642867537665718,\n",
       " 'lexical_diversity': 0.005306335008701571,\n",
       " 'top_20': [('game', 24144),\n",
       "  ('steelers', 21664),\n",
       "  ('officially', 18799),\n",
       "  ('back', 12978),\n",
       "  ('jacksonville', 12022),\n",
       "  ('follow', 9840),\n",
       "  ('fan', 8489),\n",
       "  ('away', 7968),\n",
       "  ('play', 7470),\n",
       "  ('sunday', 6914),\n",
       "  ('win', 6677),\n",
       "  ('love', 6625),\n",
       "  ('jags', 6615),\n",
       "  ('retweet', 6564),\n",
       "  ('give', 6404),\n",
       "  ('making', 6245),\n",
       "  ('juju', 6241),\n",
       "  ('catch', 6204),\n",
       "  ('help', 6008),\n",
       "  ('awesome', 5936)]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_tokens = len(sd_clean)\n",
    "unique_tokens = len(set(sd_clean))\n",
    "lex_diversity = len(set(sd_clean))/len(sd_clean)\n",
    "avg_token_len = np.mean([len(w) for w in sd_clean])\n",
    "top_20 = Counter(sd_clean).most_common(20)\n",
    "\n",
    "results = {'tokens':total_tokens,\n",
    "            'unique_tokens':unique_tokens,\n",
    "            'avg_token_length':avg_token_len,\n",
    "            'lexical_diversity':lex_diversity,\n",
    "            'top_20':top_20}\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
